{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861da5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import TypedDict\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "# Initialize HuggingFace Inference client\n",
    "client = InferenceClient(\"google/gemma-2-2b-it\", token=\"paste-your-token-here\")\n",
    "\n",
    "# Define state structure\n",
    "class LLMState(TypedDict):\n",
    "    question: str\n",
    "    answer: str\n",
    "\n",
    "# Node function\n",
    "def llm_qa(state: LLMState) -> LLMState:\n",
    "    question = state[\"question\"]\n",
    "    messages = [{\"role\": \"user\", \"content\": question}]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        max_tokens=200,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    state[\"answer\"] = response.choices[0].message[\"content\"]\n",
    "    return state\n",
    "\n",
    "# Create the graph\n",
    "graph = StateGraph(LLMState)\n",
    "graph.add_node('llm_qa', llm_qa)\n",
    "graph.add_edge(START, 'llm_qa')\n",
    "graph.add_edge('llm_qa', END)\n",
    "\n",
    "workflow = graph.compile()\n",
    "\n",
    "# Execute workflow\n",
    "initial_state = {'question': 'how far is moon from the earth'}\n",
    "final_state = workflow.invoke(initial_state)\n",
    "print(final_state)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
